{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import Dict\n",
    "import transformers\n",
    "from torch import Tensor\n",
    "from torch.nn import init, Parameter\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, average_precision_score\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e19267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int = 1992):\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(\"Using Seed Number {}\".format(seed))\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(\n",
    "        seed\n",
    "    )  # set PYTHONHASHSEED env var at fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    # set fixed value for python built-in pseudo-random generator\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea99b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMITA2i_LSTM(torch.jit.ScriptModule):\n",
    "    def __init__(self, input_size, hidden_size,seq_len, output_dim, batch_first=True, bidirectional=True):\n",
    "        super(AMITA2i_LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_dim = output_dim\n",
    "        self.initializer_range=0.02\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = bidirectional\n",
    "        self.c1 = torch.Tensor([1]).float()\n",
    "        self.c2 = torch.Tensor([np.e]).float()\n",
    "        self.ones = torch.ones([self.input_size,1, self.hidden_size]).float()\n",
    "        self.decay_features = torch.Tensor(torch.arange(self.input_size)).float()\n",
    "        self.register_buffer('c1_const', self.c1)\n",
    "        self.register_buffer('c2_const', self.c2)\n",
    "        self.register_buffer(\"ones_const\", self.ones)\n",
    "        self.alpha = torch.FloatTensor([0.5])\n",
    "        self.imp_weight = torch.FloatTensor([0.05])\n",
    "        self.alpha_imp = torch.FloatTensor([0.5])\n",
    "        self.register_buffer(\"factor\", self.alpha)\n",
    "        self.register_buffer(\"imp_weight_freq\", self.imp_weight)\n",
    "        self.register_buffer(\"features_decay\", self.decay_features)\n",
    "        self.register_buffer(\"factor_impu\", self.alpha_imp)\n",
    "        \n",
    "        self.U_j = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_f = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_c = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_last = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.U_time = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        self.Dw = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, 1, self.hidden_size)))\n",
    "        \n",
    "        self.W_j = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_f = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_c = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_d = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        self.W_decomp = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size, self.hidden_size)))\n",
    "        \n",
    "        self.W_cell_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.W_cell_f= nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.W_cell_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        \n",
    "        \n",
    "        self.b_decomp = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_j = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_i = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_f = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_o = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_c = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_last = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_time = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        self.b_d = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        # Interpolation\n",
    "        self.W_ht_mask = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size,1)))\n",
    "        self.W_ct_mask = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size,1)))\n",
    "        self.b_j_mask = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size)))\n",
    "        \n",
    "        self.W_ht_last = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size,1)))\n",
    "        self.W_ct_last = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size,1)))\n",
    "        self.b_j_last = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size, self.hidden_size))) \n",
    "        \n",
    "        #Gate Linear Unit for last records\n",
    "        self.activation_layer = nn.ELU()\n",
    "        self.F_alpha = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size,self.hidden_size*2, 1)))\n",
    "        self.F_alpha_n_b = nn.Parameter(torch.normal(0.0, self.initializer_range, size=(self.input_size,1)))\n",
    "        self.F_beta = nn.Linear(self.seq_len, 4*self.hidden_size)\n",
    "        self.layer_norm1 = nn.LayerNorm([self.input_size, self.seq_len])\n",
    "        self.layer_norm = nn.LayerNorm([self.input_size, 4*self.hidden_size])\n",
    "        self.Phi = nn.Linear(4*self.hidden_size, self.output_dim)\n",
    "        \n",
    "    @torch.jit.script_method    \n",
    "    def TLSTM_unit(self, prev_hidden_memory, cell_hidden_memory, inputs, times, last_data, freq_list):\n",
    "        h_tilda_t, c_tilda_t = prev_hidden_memory, cell_hidden_memory,\n",
    "        x = inputs\n",
    "        t = times\n",
    "        l = last_data\n",
    "        freq=freq_list\n",
    "        T = self.map_elapse_time(t)\n",
    "        # Short-term memory contribution\n",
    "        D_ST = torch.tanh(torch.einsum(\"bij,ijk->bik\", c_tilda_t, self.W_decomp))  \n",
    "        # Apply temporal decay to D-STM\n",
    "        decay_factor = torch.mul(T, self.freq_decay(freq, h_tilda_t))\n",
    "        D_ST_decayed = D_ST * decay_factor\n",
    "        # Long-term memory contribution\n",
    "        LTM = c_tilda_t - D_ST + D_ST_decayed  \n",
    "        # Combine short-term and long-term memory\n",
    "        c_tilda_t = D_ST_decayed + LTM\n",
    "        #frequency weights for imputation of missing data based on frequencies of features\n",
    "        # Imputation gate for inputs x last records\n",
    "        x_last_hidden =torch.tanh(torch.einsum(\"bij,ijk->bik\", self.freq_decay(freq, h_tilda_t),self.W_ht_last)+\\\n",
    "                                  torch.einsum(\"bij,ijk->bik\", self.freq_decay(freq, c_tilda_t),\n",
    "                                  self.W_ct_last)+self.b_j_last).permute(0, 2, 1)\n",
    "        \n",
    "        imputat_imputs = torch.tanh(torch.einsum(\"bij,ijk->bik\", self.freq_decay(freq, h_tilda_t), self.W_ht_mask)+\\\n",
    "                                    torch.einsum(\"bij,ijk->bik\",self.freq_decay(freq, c_tilda_t),self.W_ct_mask)+\\\n",
    "                                    self.b_j_mask).permute(0, 2, 1)\n",
    "        # Replace nan data with the impuated value generated from LSTM memory and frequencies weights\n",
    "\n",
    "        _, x_last = self.impute_missing_data(l, freq, x_last_hidden)\n",
    "        all_imputed_x, imputed_x = self.impute_missing_data(x, freq, imputat_imputs)\n",
    "        \n",
    "        # Ajust previous to incoporate the latest records for each feature\n",
    "        last_tilda_t = self.activation_layer(torch.einsum(\"bij,jik->bjk\", x_last, self.U_last)+self.b_last)\n",
    "        h_tilda_t = h_tilda_t + last_tilda_t\n",
    "        # Capturing Temporal Dependencies wrt to the previous hidden state\n",
    "        # Capturing Temporal Dependencies wrt to the previous hidden state\n",
    "        j_tilda_t = torch.tanh(torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_j) +\\\n",
    "                               torch.einsum(\"bij,jik->bjk\", imputed_x,self.U_j) + self.b_j)\n",
    "        \n",
    "        # Time Gate\n",
    "        t_gate = torch.sigmoid(torch.einsum(\"bij,jik->bjk\",imputed_x, self.U_time) + \n",
    "                               torch.sigmoid(self.map_elapse_time(t)) + self.b_time)\n",
    "        # Input Gate\n",
    "        i= torch.sigmoid(torch.einsum(\"bij,jik->bjk\",imputed_x, self.U_i)+\\\n",
    "                         torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_i)+\\\n",
    "                         c_tilda_t*self.W_cell_i + self.b_i*self.freq_decay(freq, j_tilda_t))\n",
    "        # Forget Gate\n",
    "        f= torch.sigmoid(torch.einsum(\"bij,jik->bjk\", imputed_x, self.U_f)+\\\n",
    "                         torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_f)+\\\n",
    "                         c_tilda_t*self.W_cell_f + self.b_f+j_tilda_t)\n",
    "\n",
    "        f_new = f * self.map_elapse_time(t) + (1 - f) *  self.freq_decay(freq, j_tilda_t)\n",
    "        # Candidate Memory Cell\n",
    "        C =torch.tanh(torch.einsum(\"bij,jik->bjk\", imputed_x, self.U_c)+\\\n",
    "                      torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_c) + self.b_c)\n",
    "        # Current Memory Cell\n",
    "        Ct = (f_new + t_gate) * c_tilda_t + i * j_tilda_t * t_gate * C\n",
    "        # Output Gate        \n",
    "        o = torch.sigmoid(torch.einsum(\"bij,jik->bjk\", imputed_x, self.U_o)+\n",
    "                          torch.einsum(\"bij,ijk->bik\", h_tilda_t, self.W_o)+\n",
    "                          t_gate + last_tilda_t + Ct*self.W_cell_o + self.b_o)\n",
    "        # Current Hidden State\n",
    "        h_tilda_t = o * torch.tanh(Ct+last_tilda_t)\n",
    "        \n",
    "        return h_tilda_t, Ct, self.freq_decay(freq, j_tilda_t), f_new, all_imputed_x\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def impute_missing_data(self, x: torch.Tensor, freq_dict: torch.Tensor, x_hidden: torch.Tensor):\n",
    "        # Calculate feature factor\n",
    "        factor_feature = torch.div(\n",
    "            torch.exp(-self.imp_weight_freq * freq_dict),\n",
    "            torch.exp(-self.imp_weight_freq * freq_dict).max()).unsqueeze(1)\n",
    "        \n",
    "        # Calculate imputation factor\n",
    "        factor_imp = torch.div(\n",
    "            torch.exp(self.factor_impu * freq_dict),\n",
    "            torch.exp(self.factor_impu * freq_dict).max()).unsqueeze(1)\n",
    "        \n",
    "        # Adjust frequencies\n",
    "        frequencies = (self.seq_len-freq_dict) * torch.exp(-self.factor * self.features_decay)\n",
    "        frequencies = torch.div(frequencies, frequencies.max()).unsqueeze(-1)\n",
    "        \n",
    "        # Compute imputed values\n",
    "        imputed_missed_x = torch.where(\n",
    "            factor_imp == factor_imp.max(),\n",
    "            frequencies.permute(0,2,1) * x_hidden,\n",
    "            factor_feature * x_hidden\n",
    "        )\n",
    "        \n",
    "        # Replace missing values\n",
    "        x_imputed = torch.where(torch.isnan(x.unsqueeze(1)), imputed_missed_x, x.unsqueeze(1))\n",
    "        \n",
    "        return imputed_missed_x, x_imputed\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def map_elapse_time(self, t):\n",
    "        T = torch.div(self.c1_const, torch.log(t + self.c2_const))\n",
    "        T = torch.einsum(\"bij,jik->bjk\", T.unsqueeze(1), self.ones_const)\n",
    "        return T\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def freq_decay(self, freq_dict: torch.Tensor, ht: torch.Tensor):\n",
    "        freq_weight = torch.exp(-self.factor_impu * freq_dict)\n",
    "        weights = torch.sigmoid(torch.einsum(\"bij,jik->bjk\",freq_weight.unsqueeze(-1),self.Dw)+\\\n",
    "                                torch.einsum(\"bij,ijk->bik\", ht, self.W_d)+ self.b_d)\n",
    "        return weights\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, inputs, times, last_values, freqs):\n",
    "        device = inputs.device\n",
    "        if self.batch_first:\n",
    "            batch_size = inputs.size()[0]\n",
    "            inputs = inputs.permute(1, 0, 2)\n",
    "            last_values = last_values.permute(1, 0, 2)\n",
    "            freqs = freqs.permute(1, 0, 2)\n",
    "            times = times.transpose(0, 1)\n",
    "        else:\n",
    "            batch_size = inputs.size()[1]\n",
    "        prev_hidden = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "        prev_cell = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "       \n",
    "        seq_len = inputs.size()[0]\n",
    "        imputed_inputs = torch.jit.annotate(List[Tensor], [])\n",
    "        hidden_his = torch.jit.annotate(List[Tensor], [])\n",
    "        weights_decay = torch.jit.annotate(List[Tensor], [])\n",
    "        weights_fgate = torch.jit.annotate(List[Tensor], [])\n",
    "        for i in range(seq_len):\n",
    "            prev_hidden, prev_cell,pre_we_decay, fgate_f, imputed_x = self.TLSTM_unit(prev_hidden,\n",
    "                                                                           prev_cell, \n",
    "                                                                           inputs[i],\n",
    "                                                                           times[i], \n",
    "                                                                           last_values[i],\n",
    "                                                                           freqs[i])\n",
    "            hidden_his += [prev_hidden]\n",
    "            imputed_inputs += [imputed_x]\n",
    "            weights_decay += [pre_we_decay]\n",
    "            weights_fgate += [fgate_f]\n",
    "        imputed_inputs = torch.stack(imputed_inputs)\n",
    "        hidden_his = torch.stack(hidden_his)\n",
    "        weights_decay = torch.stack(weights_decay)\n",
    "        weights_fgate = torch.stack(weights_fgate)\n",
    "        if self.bidirectional:\n",
    "            second_hidden = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "            second_cell = torch.zeros((batch_size, inputs.size()[2], self.hidden_size), device=device)\n",
    "            second_inputs = torch.flip(inputs, [0])\n",
    "            second_times = torch.flip(times, [0])\n",
    "            imputed_inputs_b = torch.jit.annotate(List[Tensor], [])\n",
    "            second_hidden_his = torch.jit.annotate(List[Tensor], [])\n",
    "            second_weights_decay = torch.jit.annotate(List[Tensor], [])\n",
    "            second_weights_fgate = torch.jit.annotate(List[Tensor], [])\n",
    "            for i in range(seq_len):\n",
    "                if i == 0:\n",
    "                    time = times[i]\n",
    "                else:\n",
    "                    time = second_times[i-1]\n",
    "                second_hidden, second_cell,b_we_decay,fgate_b,imputed_x_b = self.TLSTM_unit(second_hidden,\n",
    "                                                                                second_cell, \n",
    "                                                                                second_inputs[i],\n",
    "                                                                                time,\n",
    "                                                                                last_values[i],\n",
    "                                                                                freqs[i])\n",
    "                second_hidden_his += [second_hidden]\n",
    "                second_weights_decay += [b_we_decay]\n",
    "                second_weights_fgate += [fgate_b]\n",
    "                imputed_inputs_b += [imputed_x_b]\n",
    "                \n",
    "            imputed_inputs_b = torch.stack(imputed_inputs_b)\n",
    "            second_hidden_his = torch.stack(second_hidden_his)\n",
    "            second_weights_fgate = torch.stack(second_weights_fgate)\n",
    "            second_weights_decay = torch.stack(second_weights_decay)\n",
    "            weights_decay =torch.cat((weights_decay, second_weights_decay), dim=-1)\n",
    "            weights_fgate =torch.cat((weights_fgate, second_weights_fgate), dim=-1)\n",
    "            hidden_his = torch.cat((hidden_his, second_hidden_his), dim=-1)\n",
    "            imputed_inputs = torch.cat((imputed_inputs, imputed_inputs_b), dim=2)\n",
    "            prev_hidden = torch.cat((prev_hidden, second_hidden), dim=-1)\n",
    "            prev_cell = torch.cat((prev_cell, second_cell), dim=-1)\n",
    "        if self.batch_first:\n",
    "            hidden_his = hidden_his.permute(1, 0, 2, 3)\n",
    "            imputed_inputs= imputed_inputs.permute(1, 0, 2, 3)\n",
    "            weights_decay = weights_decay.permute(1, 0, 2, 3)\n",
    "            weights_fgate = weights_fgate.permute(1, 0, 2, 3)\n",
    "        \n",
    "        alphas = torch.tanh(torch.einsum(\"btij,ijk->btik\", hidden_his, self.F_alpha) + self.F_alpha_n_b)\n",
    "        alphas = alphas.reshape(alphas.size(0), alphas.size(2),\n",
    "                                alphas.size(1)*alphas.size(-1))\n",
    "        mu=self.Phi(self.layer_norm(self.F_beta(self.layer_norm1(alphas))))\n",
    "        out=torch.max(mu, dim=1).values\n",
    "        return out, weights_decay, weights_fgate, imputed_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim,seq_len, output_dim, dropout=0.2):\n",
    "        super(TimeLSTM, self).__init__()\n",
    "        # hidden dimensions\n",
    "        self.input_size = input_dim\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.output_dim= output_dim\n",
    "        # Temporal embedding MWTA_LSTM\n",
    "        self.amita_2i_lstm = AMITA2i_LSTM(self.input_size, self.hidden_size,\n",
    "                                          self.seq_len, self.output_dim) \n",
    "    def forward(self,historic_features,timestamp, last_features, features_freqs , is_test=False):\n",
    "        # Temporal features embedding\n",
    "        outputs, decay_weights, fgate, imputed_inputs = self.amita_2i_lstm(historic_features,timestamp, \n",
    "                                                                           last_features, features_freqs)\n",
    "        if is_test:\n",
    "            return decay_weights, fgate, imputed_inputs.mean(axis=2), outputs\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler:\n",
    "    def __init__(self, percentage=0.2):\n",
    "        self.percentage = percentage\n",
    "        \n",
    "    def mark_data_as_missing(self, data):\n",
    "        # Create a copy of the data to avoid modifying the original tensor\n",
    "        data_with_missing = data.clone()\n",
    "\n",
    "        # Identify the observed (non-NaN) values\n",
    "        observed_mask = ~torch.isnan(data)\n",
    "        observed_flat_indices = torch.where(observed_mask.view(-1))[0]\n",
    "\n",
    "        # Randomly sample a percentage of the observed data indices to mark as missing\n",
    "        num_samples = int(self.percentage * observed_flat_indices.size(0))\n",
    "        sampled_indices = observed_flat_indices[torch.randperm(observed_flat_indices.size(0))[:num_samples]]\n",
    "\n",
    "        # Convert flat indices to 3D indices for the original shape of the data\n",
    "        sampled_3d_indices = np.unravel_index(sampled_indices.cpu().numpy(), data.shape)\n",
    "\n",
    "        # Mark the selected observed data points as missing (NaN) in the copy of the data\n",
    "        selected_data = data[sampled_3d_indices]\n",
    "        data_with_missing[sampled_3d_indices] = torch.tensor(float('nan'))\n",
    "        \n",
    "        # Return the modified data with additional missing values\n",
    "        return selected_data, data_with_missing, sampled_3d_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca21d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedLossCalculator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def dynamic_weighted_loss(self, sampled_data, sampled_imputed_x, data_freqs, outputs, labels, criterion):\n",
    "        # Calculate mean absolute error (MAE)\n",
    "        loss_imp = torch.mean(torch.abs(sampled_data - sampled_imputed_x))\n",
    "        \n",
    "        # Normalize frequencies\n",
    "        normalized_freqs = data_freqs / torch.max(data_freqs)\n",
    "        \n",
    "        # Apply dynamic weighting using exponential decay\n",
    "        dynamic_weights = torch.exp(-0.005 * (1 - normalized_freqs))\n",
    "        \n",
    "        # Apply weighted loss calculation\n",
    "        weighted_loss_imp = loss_imp * dynamic_weights\n",
    "        weighted_loss_imp = torch.sum(weighted_loss_imp) / torch.sum(dynamic_weights)\n",
    "        \n",
    "        # Calculate prediction loss\n",
    "        prediction_loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Combine imputation loss and prediction loss\n",
    "        total_loss = prediction_loss + weighted_loss_imp\n",
    "        return weighted_loss_imp, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, mode, path, patience=3, delta=0):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError(\"Argument mode must be one of 'min' or 'max'.\")\n",
    "        if patience <= 0:\n",
    "            raise ValueError(\"Argument patience must be a positive integer.\")\n",
    "        if delta < 0:\n",
    "            raise ValueError(\"Argument delta must not be a negative number.\")\n",
    "\n",
    "        self.mode = mode\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score = np.inf if mode == 'min' else -np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def _is_improvement(self, val_score):\n",
    "        \"\"\"Return True iff val_score is better than self.best_score.\"\"\"\n",
    "        if self.mode == 'max' and val_score > self.best_score + self.delta:\n",
    "            return True\n",
    "        elif self.mode == 'min' and val_score < self.best_score - self.delta:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __call__(self, val_score, model):\n",
    "        \"\"\"\n",
    "        Return True iff self.counter >= self.patience.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._is_improvement(val_score):\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print(\"Val loss improved, Saving model's best weights.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f'Early stopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                print(f'Stopped early. Best val loss: {self.best_score:.4f}')\n",
    "                return True\n",
    "\n",
    "\n",
    "class TrainerHelpers:\n",
    "    def __init__(self, input_dim, hidden_dim, seq_length, output_dim, device, optim, loss_criterion, schedulers, num_epochs, patience_n=50, task=True):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "        self.optim = optim\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.schedulers = schedulers\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience_n = patience_n\n",
    "        self.task = task\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_acc(outputs, labels, threshold=0.5):\n",
    "        # Apply threshold to get predicted labels\n",
    "        preds = (outputs.sigmoid() > threshold).float()\n",
    "        correct = (preds == labels).float().sum()\n",
    "        accuracy = correct / labels.numel()\n",
    "        return accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def acc(predicted, label):\n",
    "        predicted = predicted.sigmoid()\n",
    "        pred = torch.round(predicted.squeeze())\n",
    "        return torch.sum(pred == label.squeeze()).item()\n",
    "\n",
    "    def train_model(self, model, train_dataloader):\n",
    "        model.train()\n",
    "        running_loss, running_corrects, mae_train = 0.0, 0, 0\n",
    "        for bi, inputs in enumerate(tqdm(train_dataloader, total=len(train_dataloader), leave=False)):\n",
    "            temporal_features, timestamp, last_data, data_freqs,labels = inputs\n",
    "            temporal_features = temporal_features.to(torch.float32).to(self.device)\n",
    "            timestamp = timestamp.to(torch.float32).to(self.device)\n",
    "            last_data = last_data.to(torch.float32).to(self.device)\n",
    "            data_freqs = data_freqs.to(torch.float32).to(self.device)\n",
    "            labels = labels.to(torch.float32).to(self.device)\n",
    "            # Sampling observed datapoints to random missing \n",
    "            sampled_data,data_with_missing, indices = data_sampler.mark_data_as_missing(temporal_features)\n",
    "            self.optim.zero_grad()\n",
    "            _, _,imputed_inputs, outputs = model(data_with_missing, timestamp,\n",
    "                                                 last_data, data_freqs, is_test=True)\n",
    "            #print(outputs.shape ,labels.shape)\n",
    "            sampled_imputed_x=imputed_inputs[indices]\n",
    "            sampled_freqs=(seq_length-data_freqs[indices])\n",
    "        \n",
    "            if self.task:\n",
    "                \n",
    "                loss_imp,loss = loss_calculator.dynamic_weighted_loss(sampled_data, sampled_imputed_x,\n",
    "                                                                      sampled_freqs, outputs.sigmoid(),\n",
    "                                                                      labels, self.loss_criterion)\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                running_loss += loss.item()\n",
    "                mae_train += loss_imp.item()\n",
    "                running_corrects += self.acc(outputs,labels)\n",
    "            else:\n",
    "                loss_imp,loss = loss_calculator.dynamic_weighted_loss(sampled_data, sampled_imputed_x,\n",
    "                                                                      sampled_freqs, outputs,labels,\n",
    "                                                                      self.loss_criterion)\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                running_loss += loss.item()\n",
    "                mae_train += loss_imp.item()\n",
    "        if self.task:\n",
    "            epoch_loss = running_loss / len(train_dataloader)\n",
    "            epoch_mae_imp = mae_train / len(train_dataloader)\n",
    "            epoch_acc = running_corrects / len(train_dataloader.dataset)\n",
    "            return epoch_mae_imp, epoch_loss, epoch_acc\n",
    "        else:\n",
    "            epoch_loss = running_loss / len(train_dataloader)\n",
    "            epoch_mae_imp = mae_train / len(train_dataloader)\n",
    "            return epoch_mae_imp, epoch_loss\n",
    "\n",
    "    def valid_model(self, model, valid_dataloader):\n",
    "        model.eval()\n",
    "        running_loss, running_corrects, mae_val = 0.0, 0, 0\n",
    "        fin_targets, fin_outputs = [], []\n",
    "        for bi, inputs in enumerate(tqdm(valid_dataloader, total=len(valid_dataloader), leave=False)):\n",
    "            temporal_features, timestamp, last_data, data_freqs,labels = inputs\n",
    "            temporal_features = temporal_features.to(torch.float32).to(self.device)\n",
    "            timestamp = timestamp.to(torch.float32).to(self.device)\n",
    "            last_data = last_data.to(torch.float32).to(self.device)\n",
    "            data_freqs = data_freqs.to(torch.float32).to(self.device)\n",
    "            labels = labels.to(torch.float32).to(self.device)\n",
    "            sampled_data,data_with_missing, indices = data_sampler.mark_data_as_missing(temporal_features)\n",
    "            with torch.no_grad():\n",
    "                _, _,imputed_inputs,outputs = model(data_with_missing, timestamp,\n",
    "                                                    last_data, data_freqs, is_test=True)\n",
    "            sampled_imputed_x=imputed_inputs[indices]\n",
    "            sampled_freqs=(seq_length-data_freqs[indices])\n",
    "            \n",
    "            if self.task:\n",
    "                loss_imp,loss = loss_calculator.dynamic_weighted_loss(sampled_data, sampled_imputed_x,\n",
    "                                                                      sampled_freqs, outputs.sigmoid(),\n",
    "                                                                      labels,\n",
    "                                                                      self.loss_criterion)\n",
    "                running_loss += loss.item()\n",
    "                mae_val += loss_imp.item()\n",
    "                running_corrects += self.acc(outputs, labels)\n",
    "                fin_targets.append(labels.cpu().detach().numpy())\n",
    "                fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "            else:\n",
    "                loss_imp,loss = loss_calculator.dynamic_weighted_loss(sampled_data, sampled_imputed_x,\n",
    "                                                                      sampled_freqs, outputs, labels,\n",
    "                                                                      self.loss_criterion)\n",
    "                mae_val += loss_imp.item()\n",
    "                running_loss += loss.item()\n",
    "                fin_targets.append(labels.cpu().detach().numpy())\n",
    "                fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "        if self.task:\n",
    "            epoch_mae_imp = mae_val / len(valid_dataloader)\n",
    "            epoch_loss = running_loss / len(valid_dataloader)\n",
    "            epoch_acc = running_corrects / len(valid_dataloader.dataset)\n",
    "            return epoch_mae_imp, epoch_loss, epoch_acc, np.vstack(fin_targets), np.vstack(fin_outputs)\n",
    "        else:\n",
    "            epoch_mae_imp = mae_val / len(valid_dataloader)\n",
    "            epoch_loss = running_loss / len(valid_dataloader)\n",
    "            mse = mean_squared_error(np.vstack(fin_targets), np.vstack(fin_outputs))\n",
    "            mae = mean_absolute_error(np.vstack(fin_targets), np.vstack(fin_outputs))\n",
    "            return epoch_mae_imp, epoch_loss , mse, mae, np.vstack(fin_targets), np.vstack(fin_outputs)\n",
    "\n",
    "    def eval_model(self, model_class, model_path,  test_dataloader):\n",
    "        # Initialize the model architecture\n",
    "        model = model_class(self.input_dim, self.hidden_dim, self.seq_length, self.output_dim).to(self.device)\n",
    "        # Load the model weights\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        fin_targets, fin_outputs = [], []\n",
    "        fin_inputs_i, inputs_outputs_i = [], []\n",
    "        all_decays, fgate_weights = [], []\n",
    "        for bi, inputs in enumerate(tqdm(test_dataloader, total=len(test_dataloader), leave=False, \n",
    "                                         desc='Evaluating on test data')):\n",
    "            temporal_features, timestamp, last_data, data_freqs,labels = inputs\n",
    "            temporal_features = temporal_features.to(torch.float32).to(self.device)\n",
    "            timestamp = timestamp.to(torch.float32).to(self.device)\n",
    "            last_data = last_data.to(torch.float32).to(self.device)\n",
    "            data_freqs = data_freqs.to(torch.float32).to(self.device)\n",
    "            labels = labels.to(torch.float32).to(self.device)\n",
    "            # Random mask observed values \n",
    "            sampled_data,data_with_missing, indices = data_sampler.mark_data_as_missing(temporal_features)\n",
    "            with torch.no_grad():\n",
    "                 _, _,imputed_inputs,outputs= model(data_with_missing, timestamp,\n",
    "                                                    last_data, data_freqs, is_test=True)\n",
    "            sampled_imputed_x=imputed_inputs[indices]\n",
    "            sampled_freqs=(seq_length-data_freqs[indices])\n",
    "            if self.task:\n",
    "                fin_outputs.append(outputs.sigmoid().cpu().detach().numpy())\n",
    "                fin_targets.append(labels.cpu().detach().numpy())\n",
    "                fin_inputs_i.append(sampled_data.cpu().detach().numpy())\n",
    "                inputs_outputs_i.append(sampled_imputed_x.cpu().detach().numpy())\n",
    "            else:\n",
    "                fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "                fin_targets.append(labels.cpu().detach().numpy())\n",
    "                fin_inputs_i.append(sampled_data.cpu().detach().numpy())\n",
    "                inputs_outputs_i.append(sampled_imputed_x.cpu().detach().numpy())\n",
    "        return fin_inputs_i, inputs_outputs_i, all_decays, fgate_weights, np.vstack(fin_targets), np.vstack(fin_outputs)\n",
    "\n",
    "    def train_validate_evaluate(self,model_class, model, model_name, train_loader, val_loader, test_loader, params, model_path):\n",
    "        best_losses, all_scores = [], []\n",
    "        es = EarlyStopping(mode='min', path=f\"{os.path.join(model_path, f'model_{model_name}.pth')}\",\n",
    "                           patience=self.patience_n)\n",
    "        for epoch in range(self.num_epochs):\n",
    "            if self.task:\n",
    "                loss_imp, loss, accuracy = self.train_model(model, train_loader)\n",
    "                eval_loss_imp, eval_loss, eval_accuracy, __, _ = self.valid_model(model, val_loader)\n",
    "                if self.schedulers is not None:\n",
    "                    self.schedulers.step()\n",
    "                print(\n",
    "                    f\"lr: {self.optim.param_groups[0]['lr']:.7f}, epoch: {epoch + 1}/{self.num_epochs}, train loss imp: {loss_imp:.8f}, train loss: {loss:.8f}, acc: {accuracy:.8f} | valid loss imp: {eval_loss_imp:.8f}, valid loss: {eval_loss:.8f}, acc: {eval_accuracy:.4f}\")\n",
    "                if es(eval_loss, model):\n",
    "                    best_losses.append(es.best_score)\n",
    "                    print(\"best_score\", es.best_score)\n",
    "                    break\n",
    "            else:\n",
    "                loss_imp, loss = self.train_model(model, train_loader)\n",
    "                eval_loss_imp, eval_loss, mse_loss, mae_loss, _, _ = self.valid_model(model, val_loader)\n",
    "                if self.schedulers is not None:\n",
    "                    self.schedulers.step()\n",
    "                print(\n",
    "                    f\"lr: {self.optim.param_groups[0]['lr']:.7f}, epoch: {epoch + 1}/{self.num_epochs}, train loss imp: {loss_imp:.8f}, train loss: {loss:.8f} | valid loss imp: {eval_loss_imp:.8f} valid loss: {eval_loss:.8f} valid mse loss: {mse_loss:.8f}, valid mae loss: {mae_loss:.8f}\")\n",
    "                if es(mse_loss, model):\n",
    "                    best_losses.append(es.best_score)\n",
    "                    print(\"best_score\", es.best_score)\n",
    "                    break\n",
    "        if self.task:\n",
    "            _, _, _, y_true, y_pred = self.valid_model(model, val_loader)\n",
    "            print(y_true.shape, y_pred.shape)\n",
    "            pr_score = average_precision_score(y_true, y_pred)\n",
    "            print(f\"[INFO] PR-AUC ON FOLD :{model_name} -  score val data: {pr_score:.4f}\")\n",
    "        else:\n",
    "            _, _, _, _, y_true, y_pred = self.valid_model(model, val_loader)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            print(\n",
    "                f\"[INFO] mse loss & mae loss on validation data Fold {model_name}: mse loss: {mse:.8f} - mae loss: {mae:.8f}\")\n",
    "        if self.task:\n",
    "            f1_scores_folds = []\n",
    "            targets, outputs, real_inputs, imputed_inputs = self._evaluate_model(model_class, \n",
    "                                                                                 f\"{os.path.join(model_path,f'model_{model_name}.pth')}\",\n",
    "                                                                                 test_loader)\n",
    "            delta, f1_scr = self.best_threshold(np.vstack(targets), np.vstack(outputs))\n",
    "            scores = self.metrics_binary(targets, outputs)\n",
    "            scores_imp = self.metrics_reg_imp(real_inputs, imputed_inputs)\n",
    "            \n",
    "            f1_scores_folds.append((delta, f1_scr))\n",
    "            all_scores.append([scores, f1_scores_folds,scores_imp])\n",
    "            \n",
    "            np.savez(os.path.join(model_path, f\"results_data_{model_name}.npz\"), \n",
    "                         auc_pr=scores,impu_scores=scores_imp, true_labels_data=np.vstack(outputs), \n",
    "                         predicted_labels_data= np.vstack(targets),real_inputs = real_inputs,\n",
    "                         imputed_inputs=imputed_inputs, folds_f1_scores= f1_scores_folds)\n",
    "            print(f\"[INFO] Results on test Folds {all_scores}\")\n",
    "        else:\n",
    "            targets, outputs, real_inputs, imputed_inputs = self._evaluate_model(model_class, \n",
    "                                                                                 f\"{os.path.join(model_path, f'model_{model_name}.pth')}\",\n",
    "                                                                                 test_loader)\n",
    "            scores = self.metrics_reg(targets, outputs, params)\n",
    "            scores_imps = self.metrics_reg_imp(imputed_inputs, real_inputs)\n",
    "            scores_imp = self.metrics_reg_imp(real_inputs, imputed_inputs)\n",
    "            all_scores.append([scores, scores_imp, scores_imps])\n",
    "            np.savez(os.path.join(model_path, f\"test_data_fold_{model_name}.npz\"), \n",
    "                      reg_scores=scores,imput_scores=scores_imp, true_labels=targets, \n",
    "                      imputs_scores=scores_imps,predicted_labels= outputs, \n",
    "                     real_x = real_inputs, imputed_x=imputed_inputs)\n",
    "            print(f\"[INFO] Results on test Folds {all_scores}\")\n",
    "        return all_scores\n",
    "\n",
    "    def _evaluate_model(self, model_class, model_path,  test_dataloader):\n",
    "        targets, predicted = [], []\n",
    "        real_inputs, imputed_inputs, all_decays, fgate_weights = [], [], [], []\n",
    "        fin_inputs_i, inputs_outputs_i, _, _, y_pred, y_true = self.eval_model(model_class, model_path,\n",
    "                                                                               test_dataloader)\n",
    "        targets.append(y_true)\n",
    "        predicted.append(y_pred)\n",
    "        imputed_inputs.append(inputs_outputs_i)\n",
    "        real_inputs.append(fin_inputs_i)\n",
    "        targets_all = [np.vstack(targets[i]) for i in range(len(targets))]\n",
    "        predicted_all = [np.vstack(predicted[i]) for i in range(len(predicted))]\n",
    "        \n",
    "        real_inputs_ =[np.hstack(real_inputs[i]) for i in range(len(real_inputs))]\n",
    "        imputed_inputs_ =[np.hstack(imputed_inputs[i]) for i in range(len(imputed_inputs))]\n",
    "        return targets_all, predicted_all, real_inputs_, imputed_inputs_\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def metrics_binary(targets, predicted):\n",
    "        scores = []\n",
    "        for y_true, y_pred, in zip(targets, predicted):\n",
    "            fpr, tpr, thresholds = roc_curve(y_pred, y_true)\n",
    "            auc_score = auc(fpr, tpr)\n",
    "            pr_score = average_precision_score(y_pred, y_true)\n",
    "            scores.append([np.round(np.mean(auc_score), 4),\n",
    "                           np.round(np.mean(pr_score), 4)])\n",
    "        return scores\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def best_threshold(y_train,train_preds):\n",
    "        delta, tmp = 0, [0, 0, 0]  # idx, cur, max\n",
    "        for tmp[0] in tqdm(np.arange(0.1, 1.01, 0.01)):\n",
    "            tmp[1] = f1_score(train_preds, np.array(y_train) > tmp[0])\n",
    "            if tmp[1] > tmp[2]:\n",
    "                delta = tmp[0]\n",
    "                tmp[2] = tmp[1]\n",
    "        print('best threshold is {:.2f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "        return delta, tmp[2]\n",
    "\n",
    "    @staticmethod\n",
    "    def adjusted_r2(actual: np.ndarray, predicted: np.ndarray, rowcount: np.int64, featurecount: np.int64):\n",
    "        return 1 - (1 - r2_score(actual, predicted)) * (rowcount - 1) / (rowcount - featurecount)\n",
    "\n",
    "    def metrics_reg(self, targets, predicted, rescale_params):\n",
    "        scores = []\n",
    "        for y_true, y_pred, in zip(targets, predicted):\n",
    "            target_max, target_min = rescale_params['data_targets_max'], rescale_params['data_targets_min']\n",
    "            targets_y_true = y_true * (target_max - target_min) + target_min\n",
    "            targets_y_pred = y_pred * (target_max - target_min) + target_min\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            n = y_true.shape[0]\n",
    "            r2 = r2_score(targets_y_true, targets_y_pred)\n",
    "            adj_r2 = self.adjusted_r2(targets_y_true, targets_y_pred, n, self.input_dim)\n",
    "            scores.append([rmse, mae, r2, adj_r2])\n",
    "        return scores\n",
    "   \n",
    "    def metrics_reg_imp(self, real, imputed):\n",
    "        scores = []\n",
    "        for y_true, y_pred, in zip(real, imputed):\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            n = y_true.shape[0]\n",
    "            adj_r2 = self.adjusted_r2(y_true, y_pred, n, self.input_dim)\n",
    "            scores.append([rmse, mae, r2, adj_r2])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "dn=\"/media/sangaria/8TB-FOLDERS/PAPER_REVIEWS_DATASETS_TASKS/72_HRS_DATA_SUBJECTS_NAN/LEARN_WITH_NOTES/NEW_AGG_FUNCS\"\n",
    "task_dataset =\"ICU_TASK_72_HOURS_DATA_32_BATCH_SIZE\"\n",
    "#task_dataset =\"HOS_TASK_72_HOURS_DATA_32_BATCH_SIZE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset_loader = np.load(os.path.join(os.path.join(dn,task_dataset),\n",
    "                                          \"train_test_data.npz\"), \n",
    "                                          allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loader = all_dataset_loader['folds_data_train_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18bc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = all_dataset_loader['folds_data_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ae263",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset_loader.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_settings = np.load(os.path.join(os.path.join(dn,task_dataset),\n",
    "                                        \"data_max_min.npz\"), \n",
    "                                         allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5172475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_settings.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabcad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max, data_min= dataset_settings['data_max'], dataset_settings['data_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "## device= 'cuda:1' \n",
    "seq_length = dataset_settings['seq_length'].item()\n",
    "input_dim = dataset_settings['input_dim'].item()\n",
    "hidden_dim, output_dim  = 128, dataset_settings['output_size'].item()\n",
    "seq_length, input_dim, hidden_dim, output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ce232",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "optimizer_config={\"lr\": 1e-3, \"betas\": (0.9, 0.98), \"eps\": 4e-09, \"weight_decay\": 5e-4}\n",
    "NUM_EPOCHS =200\n",
    "NUM_FOLDS = 10\n",
    "model_name=\"AMITA2i\".lower()\n",
    "n_patience = 80\n",
    "batch_size=64\n",
    "steps_per_epoch = int(dataset_settings['shape_data'][0] / batch_size / NUM_FOLDS)\n",
    "total_steps_per_fold = int(steps_per_epoch * NUM_EPOCHS)\n",
    "num_warmup_steps = int(0.1 * total_steps_per_fold)\n",
    "num_warmup_steps,total_steps_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6a7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amita2i = TimeLSTM(input_dim, hidden_dim, seq_length, output_dim).to(device)\n",
    "# Create an instance of the class\n",
    "data_sampler = DataSampler(percentage=0.2)\n",
    "loss_calculator = EnhancedLossCalculator()\n",
    "optimizer = torch.optim.Adam(amita2i.parameters(), **optimizer_config)\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, \n",
    "                                                               num_warmup_steps=num_warmup_steps, \n",
    "                                                               num_training_steps=total_steps_per_fold, \n",
    "                                                               num_cycles=2)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "best_model_wts = deepcopy(amita2i.state_dict())\n",
    "amita2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd956120",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_inference = TrainerHelpers(input_dim, hidden_dim, seq_length, output_dim,\n",
    "                                       device, optimizer, criterion, scheduler, NUM_EPOCHS, \n",
    "                                       patience_n=n_patience, task=True)\n",
    "train_valid_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b35a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_data_used = seq_length\n",
    "taskname=f\"ICU_MORTALITY_{hours_data_used}_HOURS_DATA\"\n",
    "main_path = \"20%_MISSING_RATE\"\n",
    "task_path=f\"{os.path.join(main_path, f'{taskname}')}\"\n",
    "if not os.path.exists(task_path):\n",
    "    os.makedirs(task_path)\n",
    "task_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac882eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_folds= []\n",
    "for idx, (train_loader, test_data) in enumerate(zip(train_val_loader ,test_loader)):\n",
    "    print(f'[INFO]: Training on fold : {idx+1}')\n",
    "    # Reset the model weights\n",
    "    amita2i.load_state_dict(best_model_wts)\n",
    "    train_data, valid_data= train_loader\n",
    "    scores= train_valid_inference.train_validate_evaluate(TimeLSTM, amita2i, idx+1,train_data,valid_data,\n",
    "                                                          test_data, dataset_settings, task_path)\n",
    "    scores_folds.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
